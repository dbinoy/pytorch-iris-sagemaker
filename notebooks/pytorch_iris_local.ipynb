{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import utils, cuda, device, from_numpy, backends, manual_seed, no_grad, save, float32, long, max\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "batch_size_train = 20\n",
    "batch_size_test = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "hl = 10\n",
    "random_seed = 1\n",
    "backends.cudnn.enabled = False\n",
    "manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data \n",
    "y = iris.target\n",
    "target_names = iris['target_names']\n",
    "feature_names = iris['feature_names']\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(target_names)\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"data\")\n",
    "except OSError:\n",
    "    print(\"Results folder exists\")\n",
    "np.save('data/iris_train_data', X_train)\n",
    "np.save('data/iris_train_target', y_train)\n",
    "np.save('data/iris_test_data', X_test)\n",
    "np.save('data/iris_test_target', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_loaded = np.load('data/iris_train_data.npy')\n",
    "y_train_loaded = np.load('data/iris_train_target.npy')\n",
    "X_test_loaded = np.load('data/iris_test_data.npy')\n",
    "y_test_loaded = np.load('data/iris_test_target.npy')\n",
    "print(X_train_loaded.shape)\n",
    "print(y_train_loaded.shape)\n",
    "print(X_test_loaded.shape)\n",
    "print(y_test_loaded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, x, y,iscuda=False):\n",
    "        self.X = np.array(x)\n",
    "        self.y = np.array(y)\n",
    "        self.cuda = iscuda\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x_val = self.X[index]\n",
    "        x_val = from_numpy(x_val)\n",
    "        y_val = from_numpy(np.array([self.y[index]]))\n",
    "        if self.cuda:\n",
    "            x_val = x_val.cuda()\n",
    "            y_val = y_val.cuda()\n",
    "        return x_val, y_val\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def close(self):\n",
    "        self.archive.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use_cuda = cuda.is_available()\n",
    "train_loader = DataLoader(\n",
    "                            IrisDataset(X_train, y_train), \n",
    "                            batch_size=batch_size_train, \n",
    "                            shuffle=True\n",
    "                        )\n",
    "test_loader = DataLoader(\n",
    "                            IrisDataset(X_test, y_test), \n",
    "                            batch_size=batch_size_train, \n",
    "                            shuffle=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_iris(loader, feature_names, target_names):\n",
    "    \n",
    "    examples = enumerate(loader)\n",
    "    batch_idx, (example_data, example_targets) = next(examples)\n",
    "    for feature_name in feature_names:\n",
    "        print(feature_name, end=\"\\t\")\n",
    "    print(\"target\")\n",
    "    for i in range(example_data.shape[0]):\n",
    "        for j in range(example_data.shape[1]):\n",
    "            print(float(example_data[i][j]), end=\"\\t\\t\\t\")\n",
    "        print(target_names[int(example_targets[i]) - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_iris(train_loader, feature_names, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(IrisClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        X = self.fc3(X)\n",
    "        X = self.softmax(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IrisClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.005\n",
    "n_epochs = 500\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = Variable(data.float())\n",
    "        target = Variable(target.reshape(target.shape[0]))\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            save(model.state_dict(), 'results/model.pth')\n",
    "            save(optimizer.state_dict(), 'results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = Variable(data.float())\n",
    "            target = Variable(target.reshape(target.shape[0]))         \n",
    "            output = model(data)\n",
    "            test_loss = test_loss + criterion(output, target)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct = correct + pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(test_loss.item())\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"results\")\n",
    "except OSError:\n",
    "    print(\"Results folder exists\")\n",
    "test(model)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(model, epoch)\n",
    "    test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (example_data, example_targets) in enumerate(test_loader):\n",
    "\n",
    "    example_data = Variable(example_data.float())\n",
    "    example_targets = Variable(example_targets.reshape(example_targets.shape[0]))   \n",
    "    with no_grad():\n",
    "        output = model(example_data)\n",
    "        for i in range(output.shape[0]):\n",
    "            print(\"Target = {}, Prediction = {}, ({})\".format(\n",
    "                    example_targets[i], \n",
    "                    output.data.max(1, keepdim=True)[1][i].item(),\n",
    "                    \"Match\" if example_targets[i] ==  output.data.max(1, keepdim=True)[1][i].item() else \"No Match\"\n",
    "                )\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
